{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from Bio import SeqIO, Seq\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Lambda, Concatenate, LSTM, MaxPooling2D, Conv2D, Flatten\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:00, 593.16it/s]/homes/ija23/advanced_ml/testenv/lib/python3.5/site-packages/Bio/Seq.py:2859: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  BiopythonWarning,\n",
      "18728it [00:25, 746.99it/s]\n"
     ]
    }
   ],
   "source": [
    "aminoacids = []\n",
    "dates      = []\n",
    "countries  = []\n",
    "for i in tqdm(SeqIO.parse('S_gene_aminoacids.fasta', 'fasta')):\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', i.id)\n",
    "    try:\n",
    "        date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "        dates.append(date)\n",
    "        aminoacids.append(str(Seq.translate(i.seq)))\n",
    "        countries.append(re.sub(r'\\/.*$','',re.sub(r'^.*?\\/', '', i.id)))\n",
    "    except:\n",
    "        aminoacids.append('0')\n",
    "        dates.append('0')\n",
    "        countries.append('0')\n",
    "    hello = deepcopy(i)\n",
    "        \n",
    "aminoacids = [x for x in aminoacids if x != '0']\n",
    "dates = [x for x in dates if x != '0']\n",
    "countries = [x for x in countries if x != '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we find unique 'characters' in the aminoacid chain, so that we can one hot encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = list(set(aminoacids[0]))\n",
    "for i, aminoacid in enumerate(aminoacids[1:]):\n",
    "    new_chars = list(set(aminoacid))\n",
    "    for char in new_chars:\n",
    "        if char in unique_chars:\n",
    "            continue\n",
    "        else:\n",
    "            unique_chars.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the aminoacid chain is 21\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique characters in the aminoacid chain is {}'.format(len(unique_chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense as we have 20 standard aminoacids + stop sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'aminoacids': aminoacids, 'dates': dates, 'country': countries}\n",
    "df = pd.DataFrame(data=a)\n",
    "df.dates = pd.to_datetime(df.dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ija23/advanced_ml/testenv/lib/python3.5/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of samples in the data, by date')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(df.groupby(by = 'dates').count().index, df.groupby(by = 'dates').count()['aminoacids'])\n",
    "plt.title('Distribution of samples in the data, by date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct dates is: 131\n"
     ]
    }
   ],
   "source": [
    "print('The number of distinct dates is: {}'.format(len(df.groupby(by = 'dates').count().index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct countries is: 118\n"
     ]
    }
   ],
   "source": [
    "print('The number of distinct countries is: {}'.format(len(df.groupby(by = 'country').count().index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['amino_length'] = df.apply(lambda row: len(row.aminoacids), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, c = np.unique(np.array(df['amino_length']), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 111,  113,  207, 1260, 1264, 1265, 1266, 1267, 1269, 1270, 1271,\n",
       "        1272, 1273, 1274, 1275]),\n",
       " array([   12,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "            4,     1,     2,    21, 18183,     3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the bar plot doesnt work on u and c, but anyway we can see that >99% of the aminoacids have length 1274, so in order to make the LSTM model easier to code (non variable feature space) **we only keep aminoacids with length 1274**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['amino_length'] == 1274]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now as discussed, we keep all the aminoacids in a given date (instead of taking unique) because we want the fact that some chains are more probable be represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If NOT then run this:\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commitment to architecture: we will treat each element of the aminoacid as a single datapoint. Its features will be a 22 long vector, first 21 items one-hot-encoding which letter it represents, the 22nd number being its position in the aminoacid chain. As we will train the LSTM we will randomly sample batches of these datapoints, bundled together in a time trajectory, with a randomly selected starting date. This will leave us with 18183*1274 = 23 165 142 datapoints so quite a number. We will train only on trajectories WITHIN the same country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary division: we train on datapoints before 2020-04-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['dates'] < '2020-04-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of distinct dates is: {}'.format(len(df_train.groupby(by = 'dates').count().index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another arbitrary decision, we will train the LSTM on trajectories (sequences) of length T, and we will train to predict the (T+1) timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we want to have sequences of at least T long, we remove countries that have less than T dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df_train.groupby('country')['dates'].nunique()\n",
    "train_countries = countries[countries >= T].index.values\n",
    "\n",
    "# Remove all other countries from df and recompute train_df\n",
    "df = df[df['country'].isin(train_countries)]\n",
    "\n",
    "df_train = df[df['dates'] < '2020-04-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of distinct dates is: {}'.format(len(df_train.groupby(by = 'dates').count().index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of distinct countries is: {}'.format(len(df_train.groupby(by = 'country').count().index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of possible changes in the amino chains per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chains = {country:np.unique(df[df['country'] == country]['aminoacids']) for country in np.unique(df['country'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = {country:np.unique(df[df['country'] == country]['aminoacids'], return_counts=True)[1] for country in np.unique(df['country'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['dates', 'country']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['country'] == 'Belgium'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in unique_chains.keys():\n",
    "    print('In {} there are {} unique chains'.format(country, len(unique_chains[country])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets now try and see where in the chain these changes happpen (on aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_chains = np.unique(df['aminoacids'])\n",
    "\n",
    "chain_position_unique = {idx:0 for idx in range(len(all_unique_chains[0]))}\n",
    "\n",
    "for idx in range(len(all_unique_chains[0])):\n",
    "    nth_char = [chain[idx] for chain in all_unique_chains]\n",
    "    chain_position_unique[idx] += len(np.unique(nth_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_with_no_change = []\n",
    "positions_with_change    = []\n",
    "\n",
    "for idx in chain_position_unique.keys():\n",
    "    if chain_position_unique[idx] > 1:\n",
    "        positions_with_change.append(idx)\n",
    "    else:\n",
    "        positions_with_no_change.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positions with no change is {}'.format(len(positions_with_no_change)))\n",
    "print('Number of positions with at least 1 change is {}'.format(len(positions_with_change)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It therefore seems logical to only consider the mutations of the positions in the list positions_with_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to architecture: now the features per datapoint will be 22 (as above) + 12, a one-hot representation of which country it is - maybe the virus evolves differently in different countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below makes life easier as we dont want to keep all 20 million datapoints lying in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataObject(object):\n",
    "\n",
    "    def __init__(self, df, timesteps, positions, unique_chars):\n",
    "\n",
    "        self.df        = df # pandas dataframe of the form as above\n",
    "        self.timesteps = timesteps # number of timesteps (agreed above to T)\n",
    "        \n",
    "        self.positions = positions # Positions to consider\n",
    "        \n",
    "        self.unique_chars = unique_chars # List of unique characters encountered in the aminoacid\n",
    "        \n",
    "        # Compute stuff we dont want to compute all over again\n",
    "        df = df.sort_values(by='dates')\n",
    "        \n",
    "        # Unique time steps\n",
    "        self.time_list = np.unique(df['dates'])\n",
    "        \n",
    "        # Unique list of countries\n",
    "        self.country_list = np.unique(df['country'])\n",
    "        \n",
    "        # Dictionary of one-hot encodings for country\n",
    "        self.country_dict = {country:np.zeros(len(self.country_list)) for country in self.country_list}\n",
    "        for i,country in enumerate(self.country_list):\n",
    "           self.country_dict[country][i] = 1\n",
    "        \n",
    "        # Dictionary of one-hot encodings for aminoacid\n",
    "        self.amino_dict = {char:np.zeros(len(self.unique_chars)) for char in self.unique_chars}\n",
    "        for i,char in enumerate(self.unique_chars):\n",
    "           self.amino_dict[char][i] = 1\n",
    "\n",
    "    def generate_batch(self, batch_size):\n",
    "        '''\n",
    "        Here we generate a batch of trajectories.\n",
    "        A Batch will be a matrix of size:\n",
    "            (batch_size, timesteps, 22+countries)\n",
    "        \n",
    "            because we agreed on a 22+countries long representation of a amino entry\n",
    "        '''        \n",
    "        # We will put all countries in a batch\n",
    "        dps_per_country = np.floor(batch_size*np.random.dirichlet([10]*len(self.country_list), 1))[0]\n",
    "        err = batch_size - dps_per_country.sum()\n",
    "        dps_per_country[np.argmax(dps_per_country)] += err\n",
    "        \n",
    "        for i, country in enumerate(self.country_list):\n",
    "            temp_df        = df[df['country'] == country]\n",
    "            temp_time_list = np.unique(temp_df['dates'])\n",
    "            \n",
    "            try:\n",
    "                start_date = np.random.choice(temp_time_list[:-(timesteps+1)])\n",
    "            except:\n",
    "                start_date = temp_time_list[0]\n",
    "                \n",
    "            # This list contains one too many timesteps (because of need to fill target)\n",
    "            temp_time_list = temp_time_list[(np.argwhere(temp_time_list == start_date)[0][0]):(np.argwhere(temp_time_list == start_date)[0][0] + (self.timesteps+1))]\n",
    "            \n",
    "            assert len(temp_time_list) == (self.timesteps+1), 'Something went wrong'\n",
    "            \n",
    "            batch_element  = np.zeros((int(dps_per_country[i]), self.timesteps, 22+len(self.country_list)))\n",
    "            target_element = np.zeros((int(dps_per_country[i]), self.timesteps + 1, 21))\n",
    "            \n",
    "            # Now we need to sample positions\n",
    "            positions = np.random.choice(self.positions, int(dps_per_country[i]))\n",
    "            \n",
    "            for i2, date in enumerate(temp_time_list):\n",
    "                \n",
    "                aminoacid = temp_df[temp_df['dates'] == date].sample() # this gives us random row from that date\n",
    "                \n",
    "                # Try to vectorize below loop, shouldnt be too hard\n",
    "                #this_vec = np.append(np.array([self.amino_dict[aminoacid.iloc[0]['aminoacids'][position]] for position in positions]))\n",
    "                \n",
    "                for i3, position in enumerate(positions):\n",
    "                    this_vec = np.append(self.amino_dict[aminoacid.iloc[0]['aminoacids'][position]], np.array([position]))\n",
    "                    this_vec = np.append(this_vec, self.country_dict[country])\n",
    "                    \n",
    "                    if i2 < (len(temp_time_list)-1):\n",
    "                        batch_element[i3,i2,:] = this_vec.astype(int)\n",
    "                \n",
    "                    target_element[i3,i2,:] = this_vec[:21].astype(int)\n",
    "                \n",
    "            if i == 0:\n",
    "                batch  = batch_element\n",
    "                target = target_element\n",
    "            else:\n",
    "                batch  = np.append(batch, batch_element, axis = 0)\n",
    "                target = np.append(target, target_element, axis = 0)\n",
    "            \n",
    "            \n",
    "        return np.array(batch).astype(float), np.array(target)[:,1:,:].astype(float)\n",
    "    \n",
    "class Batch(object):\n",
    "\n",
    "    def __init__(self, data_object, batch_size):\n",
    "\n",
    "        assert isinstance(data_object, DataObject), \"Pass an instance of DataObject to Batch\"\n",
    "        \n",
    "        self.all_data          = data_object\n",
    "        self.batch_size        = batch_size\n",
    "        self.data, self.target = data_object.generate_batch(batch_size)\n",
    "        \n",
    "    def __next__(self):\n",
    "        \n",
    "        self.data, self.target = self.all_data.generate_batch(self.batch_size)\n",
    "        \n",
    "        return self.data, self.target\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_object = DataObject(df_train, T, positions_with_change, unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = Batch(train_object, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Im not that good with Tensorflow, so model building will infer shape from a batch we give it, but its not too hard to change it to variable shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We train on sequences that are 35 long, but trained model architecture will allow longer or shorter sequences (I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3L_model(training_batch, hidden_layer_sizes = [512,256,128]):\n",
    "    '''\n",
    "    Builds a 3-lstm layer model, followed by a Dense classification layer\n",
    "    '''\n",
    "    batch_shape  = training_batch.data.shape\n",
    "    target_shape = training_batch.target.shape\n",
    "    \n",
    "    model_input = Input(batch_shape = \n",
    "                          (batch_shape[0],  # batch_size\n",
    "                           batch_shape[1],  # timesteps\n",
    "                           batch_shape[2]),  # features\n",
    "                          name=\"Input_layer\")\n",
    "    \n",
    "    decoder = LSTM(units = hidden_layer_sizes[0],\n",
    "                   dropout = 0.2, \n",
    "                   name = \"Decoder_lstm_1\",\n",
    "                   return_sequences = True)(model_input)\n",
    "    \n",
    "    decoder = LSTM(units = hidden_layer_sizes[1],\n",
    "                   dropout = 0.2, \n",
    "                   name = \"Decoder_lstm_2\",\n",
    "                   return_sequences = True)(decoder)\n",
    "    \n",
    "    decoder = LSTM(units = hidden_layer_sizes[2],\n",
    "                   dropout = 0.2, \n",
    "                   name = \"Decoder_lstm_3\",\n",
    "                   return_sequences = True)(decoder)\n",
    "    \n",
    "    # Reshape so we can put everything through a last softmax layer\n",
    "    # This is collecting batch + timesteps in one dimension\n",
    "    decoder = Lambda(lambda x: tf.reshape(x, [-1,x.shape[2]]),\n",
    "                    name = \"Reshape_before_dense\")(decoder)\n",
    "    \n",
    "    decoder = Dense(target_shape[-1], activation = 'softmax', \n",
    "                    name = 'Decoder_dense_1')(decoder)\n",
    "    \n",
    "    decoder = Lambda(lambda x: tf.reshape(x, [target_shape[0], target_shape[1], target_shape[2]]),\n",
    "                    name = 'Final_reshape')(decoder)\n",
    "    \n",
    "    model = Model(model_input, decoder)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def my_loss_seq(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, [-1, 21])\n",
    "    y_pred = tf.reshape(y_pred, [-1, 21])\n",
    "    \n",
    "    bce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    return bce(y_true, y_pred)\n",
    "\n",
    "def generate(train_batch):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        new_batch, new_target = next(train_batch)\n",
    "        \n",
    "        yield (new_batch, \n",
    "               new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_3L_model(train_batch)\n",
    "model.compile(loss = my_loss_seq, optimizer = Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"GPU is available: {}\".format(tf.test.is_gpu_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with open('3L_lstm.txt', 'w+') as f:\n",
    "            f.write('\\n The average loss for epoch {} is {:7.2f}'.format(epoch, logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '3L_lstm.h5'\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        filepath=checkpoint_path, \n",
    "                        verbose=1, \n",
    "                        save_weights_only=True,\n",
    "                        save_freq=32*100)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='loss', factor=0.5, patience=2, verbose=0, mode='auto',\n",
    "                min_delta=0.0001, cooldown=0, min_lr=0,\n",
    ")\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "                    generate(train_batch),\n",
    "                    steps_per_epoch=512,\n",
    "                    callbacks=[cp_callback, LossAndErrorPrintingCallback(), lr_callback],\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(data, steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
